{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiQh7GPBB4ab2zwsfGrN4h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Hotel Booking Cancellation Prediction\n","\n","## Data Overview\n","\n","A hotel operates two branches: a \"City Hotel\" located in the state's capital and a \"Resort Hotel\" near the coastal area. The provided dataset in CSV format encompasses reservations for these hotels with a wide array of attributes, offering a comprehensive view of each booking's characteristics.\n","\n","### Dataset Attributes:\n","\n","- **hotel**: Type of hotel (\"City Hotel\" or \"Resort Hotel\").\n","- **is_canceled**: Indicates if a booking was canceled (`1`) or not (`0`).\n","- **lead_time**: Days between the booking date and arrival date.\n","- **arrival_date_year**: Year of arrival.\n","- **arrival_date_month**: Month of arrival (January to December).\n","- **arrival_date_week_number**: Week number of arrival.\n","- **arrival_date_day_of_month**: Day of the month of arrival.\n","- **stays_in_weekend_nights**: Number of weekend nights (Saturday or Sunday) the guest stayed or booked.\n","- **stays_in_week_nights**: Number of weeknights (Monday to Friday) the guest stayed or booked.\n","- **adults**, **children**, **babies**: Number of adults, children, and babies, respectively.\n","- **meal**: Type of meal booked.\n","- **country**: Guest's country of origin (ISO 3155–3:2013 format).\n","- **market_segment**: Market segment designation.\n","- **distribution_channel**: Booking distribution channel.\n","- **is_repeated_guest**: Indicates if the booking was made by a repeated guest (`1`) or not (`0`).\n","- **previous_cancellations**: Number of previous bookings canceled by the customer before the current booking.\n","- **previous_bookings_not_canceled**: Number of previous bookings not canceled by the customer before the current booking.\n","- **reserved_room_type**, **assigned_room_type**: Codes for room types booked and assigned, respectively.\n","- **booking_changes**: Number of changes made to the booking from the time it was entered into the PMS until check-in or cancellation.\n","- **deposit_type**: Indicates if the customer made a deposit to guarantee the booking.\n","- **agent**, **company**: ID of the travel agency and company that made the booking.\n","- **days_in_waiting_list**: Days the booking was on the waiting list before being confirmed.\n","- **customer_type**: Type of booking.\n","- **adr**: Average daily rate.\n","- **required_car_parking_spaces**: Number of car parking spaces requested by the guest.\n","- **total_of_special_requests**: Number of special requests made by the guest.\n","- **reservation_status**: Last status of the booking.\n","- **reservation_status_date**: Date of the last status update.\n","- **name**, **email**, **phone**, **credit_card**: Customer's name, email, phone number, and last four digits of the credit card.\n","\n","The dataset for this analysis is sourced from `hotel_bookings_training.csv`, which is a subset of a larger dataset available on Kaggle. Further information about the dataset's origin can be explored [here](https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand).\n","\n","### Focus on Recall\n","\n","In this analysis, beyond the conventional metric of accuracy, the principal concern is the comprehensive identification of potential booking cancellations. The client prioritizes preventive measures and values the act of confirming bookings with customers — a practice seen as both acceptable and indicative of attentive customer service, even if it includes contacting those who might not eventually cancel their reservations. This emphasis on recall ensures that predictive efforts align with the client's strategy to mitigate last-minute cancellations through proactive engagement.\n","```"],"metadata":{"id":"vMblsYFvYz5i"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up the variable for your file path\n","file_path =  'data/hotel_bookings_training.csv' #or the Google Drive path"],"metadata":{"id":"GX7l6ZEH04c-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35fkuMQGz5c7"},"outputs":[],"source":["import pandas as pd\n","\n","hotel_bookings = pd.read_csv(file_path)\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["hotel_bookings.info()"],"metadata":{"id":"pDzagr0t00hP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove personal information of customers\n","hotel_bookings = hotel_bookings.drop(['name', 'email', 'phone-number', 'credit_card'], axis=1)"],"metadata":{"id":"nFCDZ3ZA09lq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hotel_bookings.sample(10)"],"metadata":{"id":"p1kf57y81J2v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##EDA"],"metadata":{"id":"SSpDZ6KP1f70"}},{"cell_type":"code","source":["!pip install pandas_profiling"],"metadata":{"id":"aAit7Uoj2fCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ydata_profiling import ProfileReport # Previously pandas_profiling # Create a report on our data in an HTML file"],"metadata":{"id":"gmuP13ks2ook"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["profile = ProfileReport(hotel_bookings, title=\"Pandas Profiling Report\")"],"metadata":{"id":"20AtsxXi2z8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install matplotlib\n","!pip install --upgrade Pillow\n","import matplotlib.pyplot as plt\n","profile.to_file(\"data/bookings_profile.html\")"],"metadata":{"id":"0g39d6yo3OPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!ls"],"metadata":{"id":"egjl3MxM3exI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('data/bookings_profile.html')"],"metadata":{"id":"zuM576pB3nYL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Avoiding Data Leakage and Handling Imbalanced Data\n","\n","It's crucial to split our dataset before applying any transformations to ensure that our model training and evaluation phases are as realistic and unbiased as possible. Here, we explain our strategy to avoid data leakage and how we address the challenge of imbalanced data."],"metadata":{"id":"5jp8HFnu8L39"}},{"cell_type":"markdown","source":["## Data Leakage\n","\n","The variable `reservation_status` indicates the status of the reservation. However, it is essentially a reflection of `is_canceled`. If we include it among the input variables, we would be leaking information to the model. This could lead to an excellent model performance, but it wouldn't be useful in real-world scenarios. Therefore, we need to remove it from the dataset along with other associated variables."],"metadata":{"id":"ked9MF_l4n2I"}},{"cell_type":"markdown","source":["Understanding Data Leakage\n","Data leakage can occur in several ways, including but not limited to:\n","\n","Forgetting to hide certain information, such as personal data, which should not be available to the model during training.\n","Using information from the test/validation set to train the model.\n","Data leakage leads to models learning patterns they shouldn't, resulting in deceptively high performance when evaluated on the same data but potentially much poorer performance on new or unseen data. This underscores the importance of careful data handling and model evaluation strategies to ensure the model's real-world applicability and reliability."],"metadata":{"id":"Vu3qj6DO5zp2"}},{"cell_type":"code","source":["# Avoid data leakage\n","hotel_bookings = hotel_bookings.drop(['reservation_status', 'reservation_status_date'], axis=1)"],"metadata":{"id":"b7FOD2p26PEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Separate the Target Variable"],"metadata":{"id":"GAmjQAXg6Zjr"}},{"cell_type":"code","source":["is_canceled = hotel_bookings['is_canceled'].copy()\n","hotel_data = hotel_bookings.drop(['is_canceled'], axis=1)"],"metadata":{"id":"I9rZkNSf6fpf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Split the Data into Training, Testing, and Validation Sets"],"metadata":{"id":"YxwRiU6P6l6V"}},{"cell_type":"code","source":["# Obtain the total number of records in the dataset\n","original_count = len(hotel_bookings)\n","\n","# Define the proportion of the dataset to allocate for training\n","training_size = 0.60  # 60% of records for training\n","\n","# Calculate the sizes for the test and validation sets, splitting the remaining data equally\n","test_size = (1 - training_size) / 2  # 20% for testing, 20% for validation\n","\n","# Calculate the actual number of records for each set based on their proportions\n","training_count = int(original_count * training_size)  # Number of records for training\n","test_count = int(original_count * test_size)  # Number of records for testing\n","validation_count = original_count - training_count - test_count  # Remaining records for validation\n","\n","# Print out the sizes for each set to verify the distribution\n","print(f\"Training count: {training_count}, Test count: {test_count}, Validation count: {validation_count}, Total: {original_count}\")\n"],"metadata":{"id":"qlU-Oiq_7Js9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Splitting the data into training data for 'hotel_data' and the target variable 'is_canceled'.\n","# The dataset is split into training and 'rest' (which includes both test and validation subsets).\n","train_x, rest_x, train_y, rest_y = train_test_split(hotel_data, is_canceled, train_size=training_count)\n","# Here, 'hotel_data' and the target variable 'is_canceled' are being split.\n","# 'train_size' is set to 'training_count' (60% of records as defined above).\n","\n","# Further split the 'rest' data into test and validation sets, each comprising 20% of the total data.\n","test_x, validate_x, test_y, validate_y = train_test_split(rest_x, rest_y, train_size=test_count)\n","# This operation splits the remaining data into test and validation subsets, based on 'test_count'.\n","\n","# Print the lengths of the training, test, and validation datasets to verify the splits.\n","print(len(train_x), len(test_x), len(validate_x))\n"],"metadata":{"id":"4CujQXNT7ik8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#One-hot encoding"],"metadata":{"id":"pnhfWcXH-aKE"}},{"cell_type":"markdown","source":["One-hot encoding is a technique for converting categorical variables (strings) into a numerical representation. In this case, it applies to the column indicating the hotel type associated with each booking.\n","\n","While Pandas provides a convenient method called get_dummies for quick analysis, it's not reproducible in a production or more formal analysis setting. Instead, it's recommended to use the OneHotEncoder from the scikit-learn library for a more robust and reproducible approach."],"metadata":{"id":"5U5CPoyr-bRa"}},{"cell_type":"markdown","source":["## Variables to Encode - One-hot Encoding"],"metadata":{"id":"ghMAJIpQAOcT"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"ec2OQhX3BeF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")"],"metadata":{"id":"Rs1m7VBrBgWq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_hot_encoder.fit(train_x[['hotel']])\n","one_hot_encoder.transform(train_x[['hotel']])"],"metadata":{"id":"NeNopVM8BkpK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When constructing a One-Hot Encoder, it's advisable to generate a sparse matrix and to ignore unknown variables/values. It's crucial to remember that the .fit method should always be applied to the training data (train_x), while the .transform method should be applied to both the testing (test_x) and validation data (validate_x).\n","\n","For ordinal variables, consider using a Label Encoder instead.\n","\n","In cases of high cardinality - where a categorical variable contains a large number of unique values - it's important to maintain information while reducing the number of categorical variables. Techniques such as embeddings or grouping can be employed to manage variables with high levels of unique values without resorting to one-hot encoding. For instance, a variable representing countries can exhibit high cardinality; in such scenarios, rather than applying one-hot encoding, more advanced techniques like embeddings or grouping countries into categorical variables (like continents) could be used to reduce cardinality effectively.\n","\n","In the context of this project, NLTK (Natural Language Toolkit) is not utilized as there are no extensive text variables to process.\n","\n","However, if your dataset includes variables with substantial text content (such as customer comments), tools and techniques for Natural Language Processing (NLP) in Python, like NLTK, can be highly effective for processing and extracting meaningful information from text data."],"metadata":{"id":"D6HiU3XECP4X"}},{"cell_type":"markdown","source":["#Binarizer"],"metadata":{"id":"4ttQjFwoCtn8"}},{"cell_type":"markdown","source":["## Variables to Binarize\n","\n"," - total_of_special_requests, required_car_parking_spaces, booking_changes, previous_bookings_not_canceled, previous_cancellations"],"metadata":{"id":"gG8aVwBxC54n"}},{"cell_type":"markdown","source":["In this scenario, the chosen approach was to binarize these variables to determine whether a client made a specific request or took a particular action, translating it into a binary format represented as a 0 or 1 value. These variables will be incorporated into the feature engineering pipeline within the binarizer column transformer."],"metadata":{"id":"58FBH1m6Ft9y"}},{"cell_type":"code","source":["from sklearn.preprocessing import Binarizer"],"metadata":{"id":"YRWb14xbDIY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["binarizer = Binarizer()"],"metadata":{"id":"iuCzwsBEDKhj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x_copy = train_x.copy()\n","\n","binarizer.fit(_[['total_of_special_requests']])\n","train_x_copy['has_made_special_requests'] = binarizer.transform(train_x[['total_of_special_requests']])\n","\n","train_x_copy[['total_of_special_requests', 'has_made_special_requests']].sample(10)"],"metadata":{"id":"JySH3pSfDN9L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of being a high-cardinality categorical variable, it is now a binary variable with two values, 1 and 0, yes and no."],"metadata":{"id":"_swVl97kDXN_"}},{"cell_type":"markdown","source":[" total_of_special_requests is not ordinal; it cannot be quantified. (Customers who have made 0, 1, 2, 3, 4, 5 special requests.)\n","\n","The requests are different from each other, and the relationship between them varies.\n","Perhaps one request was very specific and another was for two bottles of water.\n","\n","Binarizer: to determine if the customer made any requests or not. (True/False)\n","\n","(see the HTML report, Pandas Profiling Report)\n","\n","booking_changes: the number of changes requested by the customer. Perhaps we are not interested in how many changes the customer made, but whether they made any changes or not.\n","\n","previous_cancellations, previous_bookings_not_canceled: To identify someone who has made cancellations and someone who has not, regardless of the number.\n","\n","It's not to reduce the model's complexity.\n","Discarding the number of cancellations because it's not as informative.\n","Most clients did not cancel.\n","It improves the model, making it more general.\n","\n","Reducing the model's complexity also reduces the execution time during training and testing, which is more economical.\n","\n","Binarizer Documentation:\n","Specify the threshold. To determine if few or many requests were made, if more than 3, mark the requests as positive; if less, 0. binarizer = Binarizer (threshold=3)\n","\n","One might choose not to binarize a variable in another case; the total requests could be treated as ordinal in another scenario.\n"],"metadata":{"id":"ojkSLskCDwwn"}},{"cell_type":"markdown","source":["#Scaler"],"metadata":{"id":"4w7jmTbvFhl9"}},{"cell_type":"markdown","source":["## Variable to scale\n","\n"," - adr"],"metadata":{"id":"D73NWdcbFlrh"}},{"cell_type":"markdown","source":["In the scikit-learn documentation, there are general recommendations on using scalers, which can be particularly useful for variables representing how much a hotel earns when it's occupied. This can include rooms that generate profit and others that result in losses, creating a wide range of values from -6 to 5000.\n","\n","There are various scalers such as StandardScaler, MinMaxScaler (for normally distributed data), and AbsoluteScaler. For cases with skewed data and extraordinary outliers, which deviate significantly from the expected range, a different approach is recommended.\n","\n","The RobustScaler, as detailed in scikit-learn's documentation, is specifically designed to handle outliers effectively. This scaler adjusts the data in a way that is less influenced by the presence of outliers, making it a suitable choice for variables with a wide range of values and potential outlier data points."],"metadata":{"id":"q8GgHZG_GSVk"}},{"cell_type":"code","source":["from sklearn.preprocessing import RobustScaler"],"metadata":{"id":"1uKTtAWwGhfe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = RobustScaler()"],"metadata":{"id":"2p4gP2kvGjh7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x_copy = train_x.copy()\n","scaler.fit(_[['adr']])\n","train_x_copy['adr_scaled'] = scaler.transform(train_x[['adr']])\n","\n","train_x_copy[['adr', 'adr_scaled']].sample(10)"],"metadata":{"id":"FZ5ZC6j1Gmxe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The result is a smaller range that our machine learning model can handle."],"metadata":{"id":"I9IdTMKzHIZG"}},{"cell_type":"markdown","source":["Standardization is a specific form of data scaling. Generally, in the context of data preprocessing for machine learning, scaling refers to modifying the values of features (variables) to fit them onto a common scale. There are several ways to do this, with normalization and standardization being two of the most common.\n","\n","**Standardization**\n","Standardization involves rescaling data so they have a mean (μ) of 0 and a standard deviation (σ) of 1. The formula for standardizing a feature is:\n","\n","\n","### **z = (x - μ) / σ**\n","\n","where \\(x\\)  is the original value, (μ) is the mean of the feature, and\n","(σ) is the standard deviation of the feature. Standardization does not bound values to a specific range.\n","\n","**Normalization**\n","On the other hand, normalization (often referred to as min-max scaling) rescales the data to a specific range, typically 0 to 1. The formula for normalization is:\n","\n","### **xnorm = (x - x_min)/(x_max - x_min)**\n","\n","where x_min and x_max are the minimum and maximum values of the feature, respectively.\n","\n","Comparison and Usage\n","\n","Standardization vs. Normalization: The choice between standardization and normalization depends on the specific model and the context of the problem.\n","\n","Some machine learning models, like those that assume the data is **normally distributed**, may benefit more from standardization.\n","\n","Others, especially those **sensitive to the magnitude of features but that do not assume a specific distribution**, like distance-based models, may benefit more from normalization.\n","\n","Invariance of Standardization: Standardization is invariant to the scale of measurement, meaning it changes the data to a scale that is relative to the mean and standard deviation of the data, making it useful for comparisons and for models that are sensitive to variance in the data but not necessarily to the absolute magnitude.\n","\n","In summary, both standardization and normalization are important data preprocessing techniques that scale features but do so in ways that may be more suitable for different types of models and analysis problems.\n","\n","Bimodal and Multimodal Distributions\n","\n","Scaling does not affect whether a distribution is bimodal or multimodal.\n","It depends. You should delve deeper into scaling techniques to use the scaler that best fits the data.\n","**It is always recommended to scale the data regardless of its distribution.**"],"metadata":{"id":"ldFToAx1HcNa"}},{"cell_type":"markdown","source":["#No Transformation"],"metadata":{"id":"mPrJqHhjLUiM"}},{"cell_type":"markdown","source":["###Variables to Maintain in Their Original Form:\n","\n"," - stays_in_weekend_nights, stays_in_week_nights\n","\n","The approach to these variables is contingent upon the predictive model selected for implementation.\n","\n","It is essential to evaluate the nature and assumptions of the chosen model to determine whether these variables require any form of transformation or can be incorporated directly in their original state."],"metadata":{"id":"0lheaYI7LlId"}},{"cell_type":"markdown","source":["#Transformation Pipeline"],"metadata":{"id":"6noN7G4ML4D2"}},{"cell_type":"markdown","source":["The transformation pipeline groups together various transformations to be applied to the data, streamlining the preprocessing phase. This approach enables the efficient execution of multiple operations in unison, ensuring consistency across the dataset. The pipeline is particularly useful for applying specific transformations, such as one-hot encoding, to several variables simultaneously."],"metadata":{"id":"Z-xb_MO_MOwO"}},{"cell_type":"markdown","source":["Applying One-Hot Encoding\n","One-hot encoding is a crucial step in preparing categorical variables for machine learning models. This process converts categorical data into a format that can be provided to ML algorithms to improve prediction accuracy. In our pipeline, we use a ColumnTransformer to apply one-hot encoding to specified columns. The ColumnTransformer targets columns for transformation, ensuring that the encoded output is aligned with the corresponding feature in our dataset. The variables targeted for one-hot encoding in this case include: hotel, meal, distribution_channel, reserved_room_type, assigned_room_type, and customer_type. This methodical application of one-hot encoding across multiple variables enhances the model's ability to understand and utilize categorical data effectively."],"metadata":{"id":"-fJ1w_p6MULo"}},{"cell_type":"code","source":["from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import FeatureUnion, Pipeline"],"metadata":{"id":"4TcLrlTFMYNZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_hot_encoding = ColumnTransformer([\n","    (\n","        'one_hot_encode',\n","        OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n","        [\n","            \"hotel\",\n","            \"meal\",\n","            \"distribution_channel\",\n","            \"reserved_room_type\",\n","            \"assigned_room_type\",\n","            \"customer_type\"\n","        ]\n","    )\n","])"],"metadata":{"id":"qfcVJaRcMb50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["binarizer = ColumnTransformer([\n","    (\n","        'binarizer',\n","        Binarizer(),\n","        [\n","            \"total_of_special_requests\",\n","            \"required_car_parking_spaces\",\n","            \"booking_changes\",\n","            \"previous_bookings_not_canceled\",\n","            \"previous_cancellations\",\n","        ]\n","    )\n","])\n","#This one-hot encoder breaks down categorical variables into a binary format (0 and 1), effectively eliminating any hierarchy or order within the categories.\n","one_hot_binarized = Pipeline([ #both\n","    (\"binarizer\", binarizer),\n","    (\"one_hot_encoder\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")),\n","])"],"metadata":{"id":"09Mjt7RYMfQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaler = ColumnTransformer([\n","    (\"scaler\", RobustScaler(), [\"adr\"])\n","])"],"metadata":{"id":"UPchp4LrMh7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["passthrough = ColumnTransformer([\n","    (\n","        \"passthrough\",\n","        \"passthrough\",\n","        [\n","            \"stays_in_week_nights\",\n","            \"stays_in_weekend_nights\",\n","        ]\n","    )\n","])\n","#The passthrough approach allows for certain features to remain unaltered, ensuring that the original data structure is preserved for these specific variables while still benefiting from the one-hot encoding applied to other categorical variables."],"metadata":{"id":"l_u6XBb7MmeA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In machine learning workflows, efficiently managing data transformations is crucial for model performance. The process typically involves three key steps:\n","\n","* Preparation of Individual Transformers or Pipelines: For specific groups of columns, individual transformers or pipelines are prepared to handle different data types or perform specific transformations, such as one-hot encoding for categorical variables or scaling for numerical variables.\n","\n","* Integration into a Unified Feature Engineering Scheme: These transformers are then integrated into a global schema using either FeatureUnion or ColumnTransformer. This allows for the parallel application of all necessary transformations, ensuring a comprehensive and efficient feature engineering process.\n","\n","* Global Pipeline Construction: The final step involves encapsulating the entire feature engineering process and the machine learning model into a global pipeline. This global pipeline, which can be considered a 'pipeline of pipelines,' ensures a seamless workflow from data preprocessing to model training and prediction.\n","\n","This structured approach not only facilitates the management of complex data transformations but also prevents common errors such as data leakage, enhancing model development and deployment efficiency"],"metadata":{"id":"1aSeZVU9NpcP"}},{"cell_type":"markdown","source":["## Feature Engineering Pipeline with Feature Union\n","\n","This process consolidates all prior transformations, for which individual pipelines were created, into a comprehensive feature engineering pipeline. Essentially, it acts as a 'pipeline of pipelines,' effectively grouping together various transformation pipelines.\n","\n","The Feature Union object serves as the core component, facilitating the merger of all transformation elements into a unified whole. This streamlined approach ensures that all specified transformations are applied in parallel, optimizing the feature engineering process for the machine learning model."],"metadata":{"id":"xC24cyS4ORY4"}},{"cell_type":"code","source":["# Define the feature engineering pipeline\n","feature_engineering_pipeline = Pipeline(\n","    [\n","        (  # A tuple with the name 'features' and the FeatureUnion object\n","            \"features\",\n","            FeatureUnion(\n","                [\n","                    (\"categorical\", one_hot_encoding),  # An identifier/any name and the one-hot encoder\n","                    (\"categorical_binarized\", one_hot_binarized),  # Binarized categorical features\n","                    (\"scaled\", scaler),  # Scaled features\n","                    (\"pass\", passthrough),  # Features to pass through without transformation\n","                ]\n","            ),\n","        )\n","    ]\n",")"],"metadata":{"id":"Nor9g1OsOVNl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply the pipeline to the training data\n","transformed = feature_engineering_pipeline.fit_transform(train_x)\n","print(f\"Transformed shape: {transformed.shape}\")"],"metadata":{"id":"22lXPd24Otn_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transformed # It's a matrix that our model can handle."],"metadata":{"id":"_fQ7iEgWO6Ry"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model training"],"metadata":{"id":"tSJ20lQsO_8k"}},{"cell_type":"code","source":["# Get a fresh copy of the pipeline\n","from sklearn.base import clone\n","\n","feature_transformer = clone(feature_engineering_pipeline)  # Obtain an untrained copy of the pipeline.\n","\n","features_train_x = feature_transformer.fit_transform(train_x)  # Train the pipeline and use it to transform the training dataset.\n","features_validate_x = feature_transformer.transform(validate_x)  # Transform the validation dataset.\n"],"metadata":{"id":"5JdW6_fSPPfg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now have the dataset converted to numbers. We can start training the model."],"metadata":{"id":"nAmHmLcHPZJw"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","\n","model = RandomForestClassifier(n_estimators=100) # Vary n_estimators to improve recall during validation.\n","\n","model.fit(features_train_x, train_y)\n"],"metadata":{"id":"B_ofOQ0GPi8A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model validation"],"metadata":{"id":"PVP7sJ5WPmax"}},{"cell_type":"markdown","source":["Use the validation dataset to evaluate the model's performance, focusing on recall score.\n","This step involves using the trained model to determine the optimal hyperparameters for our model.\n","\n","While the algorithm uses features_train_x to adjust the model's internal parameters,\n","as data scientists, we use the validation dataset to fine-tune the **hyperparameters** for better model control.\n","\n","* TRAINING DATA: Used by the MODEL to learn internal parameters.\n","* VALIDATION DATA: Used by DATA SCIENTISTS to adjust **hyperparameters**.\n","\n","We adjust hyperparameters based on the performance metrics like recall and accuracy score, ensuring the model's effectiveness.\n"],"metadata":{"id":"_k0gv3svP30I"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, recall_score\n","\n","pred_y = model.predict(features_validate_x)\n","\n","print(accuracy_score(validate_y, pred_y))\n","print(recall_score(validate_y, pred_y))"],"metadata":{"id":"G2D9b2-QPuZO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attempting to enhance the model involves:\n","- Adjusting the hyperparameters of the **RandomForestClassifier**, specifically the **n_estimators**, during model training. Evaluate performance on model validation using recall and accuracy score to determine the optimal estimators/hyperparameters based on the best results.\n","- Exploring other models such as **Support Vector Machines, Logistic Regression**, and others could also improve outcomes.\n","\n","These results can be further improved by experimenting with different models or adjusting parameters/hyperparameters."],"metadata":{"id":"nNFFy0dnQToJ"}},{"cell_type":"markdown","source":["# Construction of the Final Pipeline\n","\n","This final pipeline encapsulates the entire data processing and modeling flow, ensuring a streamlined and reproducible approach for prediction."],"metadata":{"id":"YjJ3eRbCQ4kn"}},{"cell_type":"code","source":["final_inference_pipeline = Pipeline([\n","    # Incorporate a fresh copy of the pre-established feature engineering pipeline.\n","    (\"feature_engineering\", clone(feature_engineering_pipeline)),\n","\n","    # Model selection with pre-defined hyperparameters.\n","    (\"model\", RandomForestClassifier(n_estimators=100))\n","])"],"metadata":{"id":"UV_2hyM9Q0en"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can concatenate the training data with the validation data to create a large dataset:"],"metadata":{"id":"eMo6qqYiRMav"}},{"cell_type":"code","source":["final_training_dataset = pd.concat([train_x, validate_x])  # 95352 records in total\n","final_training_response = pd.concat([train_y, validate_y])"],"metadata":{"id":"GSgD_RwBRbiN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of using only the initial training dataset, we concatenate the training and validation datasets to create a larger dataset. This enhanced dataset is then used to train the model that will be deployed into production."],"metadata":{"id":"8RWjjg7dRkqR"}},{"cell_type":"markdown","source":["The dataset comprises the input variables, while the response includes the binary values 1 and 0."],"metadata":{"id":"XEg3yljkRvJA"}},{"cell_type":"markdown","source":["Train the final pipeline we created above with these data:"],"metadata":{"id":"On-C170-R82t"}},{"cell_type":"code","source":["final_inference_pipeline.fit(final_training_dataset, final_training_response)"],"metadata":{"id":"jiyOpvE5R_rj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model testing"],"metadata":{"id":"UGJKK316R2bX"}},{"cell_type":"markdown","source":["See how the model performs in the real world."],"metadata":{"id":"bjiSuTa_SQaH"}},{"cell_type":"code","source":["test_pred_y = final_inference_pipeline.predict(test_x)\n","\n","print(accuracy_score(test_pred_y, test_y))\n","print(recall_score(test_pred_y, test_y))"],"metadata":{"id":"aXqR9gRuSWvC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Results:\n","\n","0.8145817602147831\n","\n","0.7606827545615068\n","\n","Based on these numbers, you can compile a report for stakeholders. These are data that evaluate the model's effectiveness (because they are from a different dataset, it verifies there is no overfitting, etc.). In this case, it's a success because the results are even better than those obtained during the model training and validation process."],"metadata":{"id":"z9O-1Y8vSmc1"}},{"cell_type":"markdown","source":["## Model persistence"],"metadata":{"id":"SZfvwF56TF72"}},{"cell_type":"markdown","source":["\n","An artifact refers to any object or file created as a result of training a machine learning model. In this case, there is only one piece, the model itself. In other scenarios, when not using pipelines, there might be multiple components to deploy to production."],"metadata":{"id":"3_ueuPt-TgvF"}},{"cell_type":"code","source":["from joblib import dump\n","\n","dump(final_inference_pipeline, \"inference_pipeline.joblib\")"],"metadata":{"id":"KfwHufuwTnRS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# So, who are we addressing?"],"metadata":{"id":"qNE7bgxkUlna"}},{"cell_type":"code","source":["from joblib import load\n","\n","ultimate_inference_pipeline = load(\"inference_pipeline.joblib\")"],"metadata":{"id":"H2QSHNHxUp4h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This involves a file with 100 new clients that the hotel wants us to evaluate using the model to determine if they will cancel or not."],"metadata":{"id":"19cwby8wUsnx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up the variable for your file path\n","file_path = '/content/drive/My Drive/My Colabs/enhanced_hotel_cancellation_prediction/data/new_customers.csv'\n","#file_path = 'data/new_customers.csv'\n","\n","new_customers = pd.read_csv(file_path)\n","new_customers.head()\n","\n","new_customers['will_cancel'] = ultimate_inference_pipeline.predict(new_customers)  # adds the column 'will_cancel', indicating if a customer will cancel (1) or not (0).\n","new_customers[['proba_check_in', 'proba_cancel']] = ultimate_inference_pipeline.predict_proba(new_customers)  # 'predict_proba' provides a probability estimate of a customer canceling or not.\n","\n","# Selects the columns and sorts them in descending order by 'proba_cancel'.\n","new_customers[['name', 'phone-number', 'will_cancel', 'proba_cancel']].sort_values(by='proba_cancel', ascending=False).head(20)\n"],"metadata":{"id":"IpC-774iUv9j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's not a 100% probability, it's an estimate based on our model. The company determines what the highest percentage they consider to be significant is (it could be 60%).\n","\n","The threshold for the RandomForestClassifier model is 50%. For example, 0.51 is marked by the model as a cancellation, but maybe 40 is marked as not a cancellation.\n","\n"],"metadata":{"id":"N7tHjk1GVSiE"}},{"cell_type":"markdown","source":["Questions and considerations for improving the model and analyzing potential data leakage:\n","\n","* Prediction Interval and Confidence Levels: Generating a prediction interval for the probability that accommodates a 95% confidence level requires additional calculation and research. This can help in assessing the uncertainty in the model's predictions.\n","\n","* Graph Creation: Learning to create various types of graphs is essential for visual data analysis and interpretation. Graphical representations can provide insights into the data and model performance.\n","\n","* Data Leakage Concerns: Training a model with potential data leakage (e.g., not removing certain columns initially or using validation/test data during training) should raise suspicions if the model performs exceptionally well. If a model seems too good to be true initially, it's worth investigating for data leakage.\n","\n","* Feature Engineering and Room Assignment: Incorporating derived features, such as whether the customer received the room they requested (comparing reserved_room_type vs. assigned_room_type), can enhance model predictions. Dates of stay could also be a powerful predictive tool, suggesting the inclusion of date features in your model.\n","\n","* Cross-validation and Hyperparameter Tuning: Automated cross-validation can replace manual validation processes, making model evaluation more efficient. Hyperparameter tuning is also crucial for optimizing model performance.\n","\n","* Handling reservation_status_date: Decomposing the date into year, month, and day can capture seasonal trends or specific calendar events affecting customer decisions. However, care should be taken to avoid inadvertently introducing data leakage through this process.\n","\n","* Advanced Tools and Techniques: Exploring advanced tools that train meta-models to identify and address low-quality data can further improve model training and accuracy. Tools like Edge Impulse might offer such capabilities, though their specific functionalities should be explored further.\n","In summary, addressing these points involves careful consideration of data preprocessing, feature engineering, model evaluation techniques, and potential use of advanced tools to refine the predictive model and ensure its validity and robustness.\n","\n"],"metadata":{"id":"KkhFPtwWVYBf"}}]}